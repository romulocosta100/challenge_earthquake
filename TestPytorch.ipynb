{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils import data\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train: 260601\n",
      "len train: 86868\n"
     ]
    }
   ],
   "source": [
    "dir_data = \"/Users/romulo/Documents/Dataset/Earthquake Damage/\"\n",
    "#dir_data = \"Dataset/\"\n",
    "df_x_train = pd.read_csv(dir_data+\"train_values.csv\",index_col=\"building_id\")\n",
    "df_y_train = pd.read_csv(dir_data+\"train_labels.csv\",index_col=\"building_id\")\n",
    "df_x_test = pd.read_csv(dir_data+\"test_values.csv\",index_col=\"building_id\")\n",
    "\n",
    "# df_x_train = df_x_train[:10000]\n",
    "# df_y_train = df_y_train[:10000]\n",
    "# df_x_test = df_x_test[:10000]\n",
    "\n",
    "data_train = df_x_train.merge(df_y_train, how='left', left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"len train:\",len(data_train))\n",
    "print(\"len train:\",len(df_x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get y train\n",
    "y_train = data_train['damage_grade'].values\n",
    "\n",
    "# remove y of data_train\n",
    "data_train = data_train.drop('damage_grade', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len all: 347469\n"
     ]
    }
   ],
   "source": [
    "# let's put the train data and test data together to make get_dummies and then divide\n",
    "df_x_all = data_train.append(df_x_test)\n",
    "print(\"len all:\",len(df_x_all))\n",
    "\n",
    "# get dummies from cat columns\n",
    "cat_var = [key for key in dict(df_x_all.dtypes) if dict(df_x_all.dtypes)[key] in ['object'] ]\n",
    "df_x_all = pd.get_dummies(df_x_all, prefix=cat_var, columns=cat_var)\n",
    "\n",
    "#divide x_train and x_test\n",
    "x_train = df_x_all.iloc[:len(data_train)]\n",
    "x_test = df_x_all.iloc[len(data_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del df_x_all,df_x_train,df_y_train,data_train,df_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get x_dev and y_dev (10% from train)\n",
    "x_train, x_dev, y_train, y_dev = train_test_split( x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "y_train = np.array([x-1 for x in y_train])\n",
    "y_dev = np.array([x-1 for x in y_dev])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len x_train : 234540  len y_train: 234540 \n",
      "len x_dev   : 26061  len y_dev  : 26061 \n",
      "len x_test  : 86868\n"
     ]
    }
   ],
   "source": [
    "print(\"len x_train : %d  len y_train: %d \" %(len(x_train),len(y_train)) )\n",
    "print(\"len x_dev   : %d  len y_dev  : %d \" %(len(x_dev),len(y_dev)) )\n",
    "print(\"len x_test  : %d\" %(len(x_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepate data for pytorch\n",
    "class MyDataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, my_data, labels):\n",
    "        'Initialization'\n",
    "        self.data = my_data\n",
    "        self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return x,y\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train.values).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).long().to(device)\n",
    "\n",
    "x_dev_tensor = torch.from_numpy(x_dev.values).float().to(device)\n",
    "y_dev_tensor = torch.from_numpy(y_dev).long().to(device)\n",
    "\n",
    "my_train = MyDataset(x_train_tensor,y_train_tensor)\n",
    "my_dev = MyDataset(x_dev_tensor,y_dev_tensor )\n",
    "\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(my_train, batch_size=64, shuffle=True)\n",
    "devloader = torch.utils.data.DataLoader(my_dev, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,IN_DIM,num_class):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(IN_DIM, 500)\n",
    "        self.fc2 = nn.Linear(500, 300)\n",
    "        \n",
    "        self.fc3 = nn.Linear(300, 300)\n",
    "        self.fc4 = nn.Linear(300, 200)\n",
    "        self.fc5 = nn.Linear(200, 100)\n",
    "        self.fc6 = nn.Linear(100, 30)\n",
    "        \n",
    "        self.fc7 = nn.Linear(30, num_class)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(F.relu(self.fc6(x)))\n",
    "        \n",
    "        x = F.log_softmax(self.fc7(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the network, define the criterion and optimizer\n",
    "_,IN_DIM = my_dev.data.shape\n",
    "num_class = 4\n",
    "model = Classifier(IN_DIM,num_class)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 8.704..  Dev Loss: 0.916..  Dev Accuracy: 0.568\n",
      "Epoch: 2/30..  Training Loss: 8.210..  Dev Loss: 0.916..  Dev Accuracy: 0.568\n",
      "Epoch: 3/30..  Training Loss: 8.200..  Dev Loss: 0.915..  Dev Accuracy: 0.568\n",
      "Epoch: 4/30..  Training Loss: 8.200..  Dev Loss: 0.915..  Dev Accuracy: 0.568\n",
      "Epoch: 5/30..  Training Loss: 8.200..  Dev Loss: 0.915..  Dev Accuracy: 0.568\n",
      "Epoch: 6/30..  Training Loss: 8.200..  Dev Loss: 0.916..  Dev Accuracy: 0.567\n",
      "Epoch: 7/30..  Training Loss: 8.200..  Dev Loss: 0.915..  Dev Accuracy: 0.568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-65d4a0b7e37c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_ps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for points, labels in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(points)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for points, labels in devloader:\n",
    "                log_ps = model(points)\n",
    "                test_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(devloader))\n",
    "        test_losses.append(test_loss/len(devloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(devloader)),\n",
    "              \"Dev Loss: {:.3f}.. \".format(test_loss/len(devloader)),\n",
    "              \"Dev Accuracy: {:.3f}\".format(accuracy/len(devloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
